{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Memory Collector","text":"<p>A Kubernetes-native collector for monitoring memory subsystem interference between pods. This project is under active development and we welcome contributors to help build this critical observability component.</p>"},{"location":"#overview","title":"Overview","text":"<p>Memory Collector helps Kubernetes operators identify and quantify performance degradation caused by memory subsystem interference (\"noisy neighbors\") by collecting metrics about:</p> <ul> <li>Memory bandwidth utilization</li> <li>Last Level Cache (LLC) usage</li> <li>CPU performance counters related to memory access</li> </ul> <p>This data helps operators: - Identify when pods are experiencing memory subsystem interference - Quantify the performance impact of noisy neighbors - Build confidence before deploying memory interference mitigation solutions</p>"},{"location":"#why-this-matters","title":"Why This Matters","text":"<p>Memory subsystem interference can cause: - 25%+ increase in cycles per instruction (CPI) - 4x-13x increase in tail latency - Reduced application performance even with CPU and memory limits</p> <p>Common sources of interference include: - Garbage collection - Big data analytics - Security scanning - Video streaming/transcoding - Container image decompression</p>"},{"location":"#development-status-contributing","title":"Development Status &amp; Contributing","text":"<p>The project is in active development across several areas:</p>"},{"location":"#core-metrics-collection","title":"Core Metrics Collection","text":"<ul> <li>Implementing collection for Intel RDT and AMD QoS</li> <li>Collecting hardware performance counters: cycles, instructions, cache misses</li> <li>Defining Prometheus metrics</li> </ul>"},{"location":"#kubernetes-integration","title":"Kubernetes Integration","text":"<ul> <li>Helm chart, DaemonSet implementation</li> <li>Prometheus integration</li> </ul>"},{"location":"#testing-documentation","title":"Testing &amp; Documentation","text":"<ul> <li>Architecture documentation</li> <li>Benchmark suite with example workloads</li> <li>Integration testing framework</li> </ul>"},{"location":"#get-involved","title":"Get Involved","text":"<p>We welcome contributions! Here's how you can help:</p> <ul> <li>Code: Check our Good First Issues and Development Guide</li> <li>Use Cases: Share interference scenarios, test in your environment</li> <li>Discussion: Open GitHub Issues or email yonch@yonch.com</li> <li>Schedule a chat: https://yonch.com/collector</li> </ul>"},{"location":"#project-background","title":"Project Background","text":"<p>This project builds on research and implementation from: - Google's CPI\u00b2 system - Meta's Resource Control implementation - Alibaba Cloud's Alita system - MIT's Caladan project</p>"},{"location":"#license","title":"License","text":""},{"location":"#code","title":"Code","text":"<p>Licensed under the Apache License, Version 2.0</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Documentation is licensed under a Creative Commons Attribution 4.0 International License.</p>"},{"location":"collection/","title":"Memory Collector Telemetry Strategy","text":""},{"location":"collection/#overview","title":"Overview","text":"<p>Our telemetry strategy prioritizes high-resolution, low-level data collection to build a foundation for understanding memory subsystem interference. By focusing on simplicity and data quality in the initial collector, we can enable rapid iteration and validation of detection algorithms.</p> <p>The key aspects of our approach are:</p> <ul> <li>Collect per-process, per-core metrics at 1 millisecond granularity to capture interference at a meaningful timescale</li> <li>Collect per-process cache occupancy metrics at 1 millisecond granularity</li> <li>Generate synchronized datasets for joint analysis</li> <li>Implement in stages to manage complexity</li> </ul> <p>This \"firehose\" telemetry will enable us to build a dataset for offline analysis, allowing us to identify patterns and develop algorithms for real-time interference detection.</p>"},{"location":"collection/#telemetry-collection","title":"Telemetry Collection","text":"<p>The collector will monitor and record the following metrics for each process at 1 millisecond granularity:</p> <ul> <li>Process ID</li> <li>Core ID </li> <li>Core frequency during the measured interval</li> <li>Cycles </li> <li>Instructions</li> <li>Last level cache misses</li> </ul> <p>Modern cloud environments routinely run dozens or even hundreds of applications on a single server, each with its own dynamic memory usage patterns. In an extreme case, with 100 applications changing phase every second on average, there would be a phase change every 10 milliseconds in aggregate.</p> <p>The 1 millisecond telemetry granularity enables us to detect this behavior and characterize interference at a meaningful timescale.</p> <p>In addition to these per-process metrics, we will also collect cache occupancy measurements using Intel RDT's Cache Monitoring Technology (CMT) or an equivalent mechanism. This data will be collected per process at the same 1 millisecond granularity.</p> <p>Monitoring cache usage per process is necessary because caches maintain state across context switches and are shared by all threads of a process.</p>"},{"location":"collection/#data-format","title":"Data Format","text":"<p>For the initial version, telemetry will be written to CSV files to simplify data collection and analysis. Each row will represent a single measurement interval for a specific process.</p> <p>We will generate two datasets:</p> <ol> <li>Per-process, per-core measurements (process ID, core ID, frequency, cycles, instructions, LLC misses)</li> <li>Per-process cache occupancy measurements</li> </ol> <p>While these datasets will be separate, they will be synchronized and aligned by timestamp to enable joint analysis.</p>"},{"location":"collection/#implementation-stages","title":"Implementation Stages","text":"<p>To manage complexity, we will implement telemetry collection in two stages:</p> <ol> <li>Collect per-process, per-core measurements (process ID, core ID, frequency, cycles, instructions, LLC misses)</li> <li>Add per-process cache occupancy measurements using Intel RDT or an equivalent mechanism</li> </ol> <p>This staged approach allows us to validate the core telemetry pipeline before adding the complexity of cache monitoring.</p> <p>For the cache monitoring stage, we will need to assign each process a unique identifier (e.g., CLOS for Intel RDT) to track its cache usage. This will require additional system-level coordination and metadata management.</p>"},{"location":"collection/#analysis-and-algorithm-development","title":"Analysis and Algorithm Development","text":"<p>By collecting high-resolution telemetry from multiple clusters, both real-world deployments and benchmark environments, we aim to build a representative dataset capturing a wide range of interference scenarios.</p> <p>Analyzing this data offline using big data techniques will help us identify common interference patterns, resource usage signatures, and relevant metrics for detecting contention.</p> <p>These insights will inform the development of algorithms for real-time interference detection in future collector versions. Starting with a thorough understanding of low-level behavior is key to building effective higher-level detection and mitigation strategies.</p>"},{"location":"design/","title":"Memory Collector Design","text":""},{"location":"design/#rmid-allocation-semantics","title":"RMID Allocation Semantics","text":"<p>The Memory Collector uses Resource Monitoring IDs (RMIDs) to track memory usage of processes. To ensure accurate measurement attribution, the RMID allocation system implements the following semantics:</p>"},{"location":"design/#rmid-lifecycle","title":"RMID Lifecycle","text":"<ol> <li>Allocation</li> <li>RMIDs are allocated to thread group leaders (processes)</li> <li>All threads within a process share the same RMID</li> <li>RMID 0 is reserved and considered invalid</li> <li> <p>Allocation fails if no RMIDs are available that have been free long enough</p> </li> <li> <p>Deallocation</p> </li> <li>RMIDs are freed when a process terminates</li> <li>The free timestamp is recorded to enforce the limbo period</li> <li> <p>Freed RMIDs are added to a FIFO queue for reuse</p> </li> <li> <p>Limbo Period</p> </li> <li>A minimum wait time of 2ms is enforced between RMID deallocation and reallocation</li> <li>This ensures measurement intervals (1ms) remain unambiguous</li> <li>Prevents the ABA problem where measurements from different processes could be mixed</li> </ol>"},{"location":"design/#measurement-guarantees","title":"Measurement Guarantees","text":"<ol> <li>Temporal Isolation</li> <li>Each RMID uniquely identifies a single process during any 1ms measurement window</li> <li>The 2ms limbo period ensures no overlap between processes using the same RMID</li> <li> <p>Userspace can safely aggregate measurements using RMID-indexed arrays</p> </li> <li> <p>Resource Efficiency</p> </li> <li>RMIDs are a limited resource (typically 512 maximum)</li> <li>The FIFO reuse policy aims to let cache footprints associated with freed RMIDs to decay before reuse.</li> <li> <p>The limbo period is kept minimal (2ms) to maintain high RMID availability. If we see high jitter in measurement timers, we can increase the limbo period.</p> </li> <li> <p>Hardware Integration</p> </li> <li>On systems with hardware RDT support, RMIDs are programmed into MSRs</li> <li>On systems without RDT support, RMIDs are emulated for consistent behavior</li> <li>Context switches update RMIDs in hardware when necessary</li> </ol>"},{"location":"design/#implementation-details","title":"Implementation Details","text":"<ol> <li>Data Structures</li> <li><code>struct rmid_info</code>: Tracks RMID metadata including process info and free timestamp</li> <li> <p><code>struct rmid_alloc</code>: Global allocator with free list (used as a queue) and spinlock protection</p> </li> <li> <p>Concurrency</p> </li> <li>Spinlock protection for all RMID operations</li> <li> <p>Lock-free fast path for thread RMID inheritance</p> </li> <li> <p>Monitoring</p> </li> <li>Tracepoints report RMID allocation and deallocation events to the eBPF collector</li> <li>Procfs interface for dumping current RMID assignments (so the eBPF collector can see RMIDs for processes that existed before the collector was loaded)</li> </ol>"},{"location":"devlog/","title":"Devlog","text":"<p>Documentation of development steps, environment, and dependencies  </p> <ul> <li>Contributors: atimeofday</li> <li>Goals: Create skeleton collector with Prometheus endpoint</li> <li>Issues: https://github.com/perfpod/memory-collector/issues/19</li> </ul> <p>Initial environment and tools:</p> <pre><code># Shell: Bash\ndistrobox create --image fedora:40 --name memory-collector \ndistrobox enter memory-collector\nsudo dnf install git go\n\n# cd to preferred project directory\n# Clone (fork of) project\ngit clone https://github.com/perfpod/memory-collector\ncd memory-collector\n</code></pre> <p>Issue 19 objective 1: Create a <code>main.go</code> file in <code>cmd/collector</code></p> <pre><code>mkdir -p cmd/collector\ncd cmd/collector\ntouch main.go\n</code></pre> <ul> <li>Prometheus client_golang reference guide: https://prometheus.io/docs/guides/go-application/</li> <li>Go package installation reference: https://go.dev/doc/go-get-install-deprecation</li> <li>Go Module reference: https://go.dev/ref/mod#go-mod-init</li> <li><code>go get</code> and <code>go install</code> require a Go Module and/or @version tag as of Go 1.17 in August 2021</li> <li>Prometheus go_client installation instructions appear to be outdated and missing a piece</li> <li>Submitted issue to Prometheus documentation repository: https://github.com/prometheus/docs/issues/2556#issue-2736636166</li> <li>Proceeded with Prometheus client_golang guide </li> </ul> <pre><code>cd cmd/collector\ngo mod init memory-collector\ngo get github.com/prometheus/client_golang/prometheus\ngo get github.com/prometheus/client_golang/prometheus/promauto\ngo get github.com/prometheus/client_golang/prometheus/promhttp\n</code></pre> <p>Issue 19 objective 2: Expose an endpoint on a known fixed port </p> <pre><code># Wrote and tested example Go exposition application from Prometheus guide\ngo run main.go &amp;\ncurl http://localhost:2112/metrics\n</code></pre> <p>Issue 19 objective 3: Expose the <code>up</code> metric with value 1</p> <pre><code>// Created, registered, and set an 'up' metric in func main()\n\nupMetric := prometheus.NewGauge(prometheus.GaugeOpts{\n    Namespace:  \"perfpod\",\n    Subsystem:  \"memory_collector\",\n    Name:       \"up_metric\",\n    Help:       \"Test metric to confirm skeleton application functionality.\",\n})\nprometheus.MustRegister(upMetric)\n\nupMetric.Set(1)\n</code></pre> <p>Issue 19 objective 4: Manually verify: query the endpoint using <code>curl</code> or <code>wget</code></p> <pre><code>curl -s http://localhost:2112/metrics | grep up_metric\n</code></pre> <p>Output:</p> <pre><code># HELP perfpod_memory_collector_up_metric Test metric to confirm skeleton application functionality.\n# TYPE perfpod_memory_collector_up_metric gauge\nperfpod_memory_collector_up_metric 1\n</code></pre> <p>Issue 19 objective 5: Move the code into a function (not <code>main()</code>)</p> <pre><code>// Moved Up metric into \"func recordMetrics()\" and added function call in main()\n\nfunc main() {\n    recordMetrics()\n\n    http.Handle(\"/metrics\", promhttp.Handler())\n    http.ListenAndServe(\":2112\", nil)\n}\n\n// Repeated manual verification endpoint query\n</code></pre> <p>Issue 19 objective 6: Add an integration test that verifies the metrics are up, using client_golang's testutil - TO DO - May require assistance</p> <ul> <li>Issue 19 split into 5/5 done and new Issue 20</li> <li>Issue 19 5/5 PR opened and merged</li> </ul> <ul> <li>Contributors: atimeofday</li> <li>Goals: Add integration test to Prometheus endpoint</li> <li>Issues: https://github.com/perfpod/memory-collector/issues/20</li> </ul> <p>Research &amp; references:</p> <pre><code>https://go.dev/doc/tutorial/add-a-test\nhttps://albertmoreno.dev/posts/testing-prometheus-metrics-in-integration-tests-in-golang/\nhttps://github.com/prometheus/client_golang/blob/main/prometheus/testutil/testutil.go\nhttps://github.com/prometheus/client_golang/blob/main/prometheus/testutil/testutil_test.go\n</code></pre> <pre><code>go get github.com/prometheus/client_golang/prometheus/testutil \ngo get github.com/stretchr/testify/require\n</code></pre> <p>Go test format:</p> <pre><code>[filename]_test.go\n\nimport(\n    [...]\n)\n\nfunc [TestFunction](t *testing.T) {\n    // Set test values\n    // Perform test\n}\n\n// Perform more tests\n</code></pre> <ol> <li>Created skeleton test based on examples </li> </ol> <pre><code>func TestMetricsUp(t *testing.T) {\n    require.Eventuallyf(t, func() bool {\n\n        // Test values\n        // ??? expected format\n\n        if err := testutil.ScrapeAndCompare(serverURL+\"/metrics\", strings.NewReader(expected), metricName); err == nil {\n            return true\n        } else {\n            t.Log(err.Error())\n            return false\n        }\n    }, time.Second, 100*time.Millisecond, \"Could not find metric %s with value %d\", metricName, expectedMetricValue)\n}\n</code></pre> <ol> <li>Checked the implementation of the testutil ScrapeAndCompare function, and notably, the implementation of its own integration test.</li> <li>Located and implemented the exact input template required by the function, then implemented generalized code for the template.</li> <li>Researched goroutines to allow automatically initializing the (currently local) remote server to be tested.</li> </ol> <pre><code>go main()\ntime.Sleep(1 * time.Second)\n</code></pre> <ol> <li>Refined logical flow from example code for improved readability.</li> </ol> <ul> <li>Issue 20 done</li> </ul> <ul> <li>Contributors: </li> <li>Goals: </li> <li>Issues: </li> </ul>"},{"location":"noise-generators/","title":"Synthetic Noise Generators","text":"<p>Synthetic noise generators are programs that cause noisy-neighbor stress on shared resources. In our case, this would be memory bandwidth and cache.</p> <p>Noise generators can be used for: 1. Testing that metrics the collector outputs are correct 2. When used in a cluster running a multi-service workload like microservices-demo, shows how the services behave under noisy neighbor. 3. Can potentially be used in staging environments to get an indication how a company's real workload would behave under noisy neighbor.</p>"},{"location":"noise-generators/#conclusion","title":"Conclusion","text":"<p>Intel MLC is the best fit, given its wide configurability via the command line, and extensive documentation. The shortcoming is that it is not open source. Installing it requires accepting the license, which might be hard automatically from a CI system, so might require some investment in automation.</p>"},{"location":"noise-generators/#top-contenders","title":"Top contenders","text":""},{"location":"noise-generators/#intel-memory-latency-checker-intel-mlc","title":"Intel Memory Latency Checker (Intel MLC)","text":"<p>Binary distribution (does not seem to be OSS). Is able to gather multiple baselines: - Idle latency and maximum bandwidth, for each source and destination NUMA node - Peak memory bandwidth at different read-write ratios - Latencies under different bandwidth loads</p> <p>The binary also disables prefetches for the duration of the run for more accurate results.</p> <ul> <li>It is possible to select the delay between memory accesses in the bandwidth generators (<code>-d</code> parameter)</li> <li>Can specify array sizes for bandwidth generation (<code>-b</code>)</li> <li>A latency-measurement thread generates dependent accesses, it might be reusable to measure a histogram of latencies on a live system (without generating bandwidth noise), would need further investigation. The idle_latency mode might do exactly that.</li> <li>Control what CPUs allocate memory, and where latency measurements run. (<code>-i</code>, <code>-j</code>)</li> <li>Can specify which CPUs are used for bandwidth measurement (<code>-k</code>)</li> <li>Ensures entire CPUs are not put in lower frequency because all cores are idle by 100% utilizing a core on each CPU (<code>-p</code>)</li> <li>In AVX512, can request explicit flushing of cache lines to DRAM (<code>-P</code>, <code>-Q</code>)</li> <li>There is an L3 bandwidth measurement mode that tests just the bandwidth to read from L3 (<code>-u</code>)</li> <li>Controlling random vs sequential access, both in latency-measurement threads and bandwidth-generation threads</li> <li>Read to write ratio (<code>-W</code>, <code>-R</code>)</li> </ul>"},{"location":"noise-generators/#pmbw-github","title":"pmbw (GitHub)","text":"<p>A C/C++ noise generator for cache and RAM, with the access loops coded in assembly. Tests have several configuration options, to achieve different stress patterns: - Sequential scanning versus walking permutations - Read or Write - Number of bits transferred per instruction, from 16 up to 256 via SSE/MMX/AVX - Accessing components using pointers versus index-accessing arrays - Regular tests, or tests with unrolled loop (I assume, to stress the instruction cache or to avoid branches on every iteration)</p> <p>Appears relatively easy to compile, since it only requires the <code>pthreads</code> and <code>rt</code> libraries.</p>"},{"location":"noise-generators/#also-considered","title":"Also considered","text":""},{"location":"noise-generators/#sysbench","title":"Sysbench","text":"<p>Mentions it is mostly used for database benchmarks, but its README mentions <code>memory</code>, a memory access benchmark. The README also documents general purpose command line parameters like the number of threads and warmup time, but a quick scan did not generate more documentation for the <code>memory</code> benchmarks.</p> <p>We found it less likely to be a fit, given its database focus and lack of documentation for the memory benchmark.</p>"},{"location":"noise-generators/#stream","title":"STREAM","text":"<p>Mature package, earliest submissions from 1991 (latest update to website benchmark 2017). Seems to be a single C and single FORTRAN source file, with a Makefile. There is very little control over the measured pattern and results are very concise: main tuning point is the size of array being measured <code>STREAM_ARRAY_SIZE</code>, and results are printed with:</p> <pre><code>    printf(\"Function    Best Rate MB/s  Avg time     Min time     Max time\\n\");\n</code></pre> <p>A recent Intel mention shows how to compile an optimized version of STREAM.</p> <p>STREAM was used in the Themis paper.</p> <p>We found it less likely to be a fit, given its lack of control over the stress pattern and results.</p>"},{"location":"noise-generators/#mmatyasbandwidth-benchmark","title":"mmatyas/bandwidth-benchmark","text":"<p>This is a memory and network bandwidth benchmark. The source code indicates development 2005-2016, with many versions, but there is little activity on GitHub. Supports SSE/AVX and random access. Earlier tests run 5 second tests for a total of 35 minutes. CSV output.</p> <p>This could be a fit, but has less documentation than the alternatives.</p>"},{"location":"noise-generators/#cachebench","title":"Cachebench","text":"<p>While used in Alita as LLC polluter, the program seems to have been developed in 1998 at University of Tennessee at Knoxville and the repo has not received additional contributions since. It has a README and pdf guide.</p>"},{"location":"noise-generators/#ibench","title":"iBench","text":"<p>Appears to have been used in multiple papers Paragon, Seer, Quasar, FIRM (alongside <code>pmbw</code>), and PARTIES. It is described in a paper.</p> <p>The repo appears to contain just 7 of the 15 stressors described in the iBench paper. Its memory bandwidth stressor seems to be less extensive than the memory access functions in <code>pmbw</code> (e.g., 860 lines of code in funcs_x86_64.g vs. 87 lines in memBw.c). The memory bandwidth benchmark only receives the length of the benchmark as a parameter, and it is unclear how to adjust the stress intensity.</p>"},{"location":"architecture/decisions/2024-12-21-container-notifications/","title":"2024-12-21: Notifications for container lifecycle events","text":""},{"location":"architecture/decisions/2024-12-21-container-notifications/#status","title":"Status","text":"<p>Draft, WIP</p>"},{"location":"architecture/decisions/2024-12-21-container-notifications/#context","title":"Context","text":"<p>We'd like the collector to show how memory resource contention influences container performance.</p> <p>To do that, we'd need to monitor: 1. Resource contention - can do this with <code>resctrl</code>, or by monitoring LLC Misses using perf counters 2. Container performance - current plan is to do this by monitoring CPI (cycles per instruction)</p> <p>For CPI monitoring, we'd need to have an inventory of containers on the system, and correctly instrument them as they arrive/go. In this issue, we add a component to monitor the arrival and departure of containers in the system.</p>"},{"location":"architecture/decisions/2024-12-21-container-notifications/#options-considered","title":"Options considered","text":""},{"location":"architecture/decisions/2024-12-21-container-notifications/#kubelet-api","title":"Kubelet API","text":"<p>If we're focusing on Kubernetes, kubelet provides an HTTP API accessible locally. This appears to be an undocumented, unstable API, that is nevertheless available in kubelet.</p> <p>Stack overflow discussion points to a project kubeletctl. The referenced blog post shows several <code>curl</code> commands to interact with the API. According to the blog post, this is available because the default kubelet configuration allows for anonymous (unauthenticated) requests, so this relies on users not fortifying their systems to this vulnerability. The specific implementation in kubeletctl appears a thin implementation of HTTP calls, so it might be best to reimplement this in our on library rather than take a dependency.</p> <p>Pros: - Should provide metadata on Pods, not only containers - Does not rely on a specific container runtime (docker, containerd, etc.)</p> <p>Cons: - Undocumented, unstable API - Requires access to kubelet, which may not be available in all environments - Appears to require polling (no <code>watch</code>). If so, will react slowly and incur more overhead.</p>"},{"location":"architecture/decisions/2024-12-21-container-notifications/#filesystem-watch-on-the-cgroup-directory-eg-inotify","title":"Filesystem watch on the cgroup directory (e.g., <code>inotify</code>)","text":"<p>This is the method used by Koordinator.sh in its PLEG component. It watches the cgroup root path for each of the Kubernetes QoS classes, for new pod directories. A new pod directory adds that pod subdirectory to a container watcher, which then issues container events.</p> <p>Pros: - Does not require access to kubelet - Does not depend on a container runtime - ABI is stable and well-documented - Supports inotify, which is efficient and low-overhead</p> <p>Cons: - Does not provide metadata beyond the pod and container IDs</p>"},{"location":"architecture/decisions/2024-12-21-container-notifications/#cri-container-runtime-interface-events","title":"CRI (Container Runtime Interface) events","text":""},{"location":"architecture/decisions/2024-12-21-container-notifications/#kubernetes-api-ie-watching-the-control-plane","title":"Kubernetes API (i.e., watching the control plane)","text":""},{"location":"architecture/decisions/2024-12-21-container-notifications/#decision","title":"Decision","text":""},{"location":"architecture/decisions/2025-01-02-aws-test-runners/","title":"2025-01-02: AWS test runners","text":""},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#context","title":"Context","text":"<p>We'd like to run some tests on AWS to check the availability of PMC (Performance Monitoring Counters) and Linux resctrl on different instance types. To do this, we'll want an automated way to run tests on different instance types.</p> <p>As of writing, the main check will be <code>cpi-count</code>, which checks the availability of cycles and instructions, and compares the results of <code>go-perf</code> and <code>perf</code> to sanity-check the results. </p> <p>In the future, we'll want to add more tests and similarly run them on different instance types. For example:</p> <ul> <li>Checking other counters than cycles and instructions (e.g., LLCMisses)</li> <li>Checking the availability of <code>resctrl</code> in Linux</li> <li>Verifying <code>resctrl</code> is able to control memory bandwidth and cache allocation</li> </ul> <p>This decision is about individual, relatively simple checks that run on a single instance. Tests that require complex workloads (e.g., DeathStarBench) are out of scope for this decision.</p>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#options-considered-ec2-based","title":"Options considered - EC2 based","text":""},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#common-pros-and-cons","title":"Common pros and cons","text":"<p>Pros:</p> <ul> <li>Easy to run multiple instances</li> <li>Gives control over the operating system and AMI, if we need that control in the future.</li> <li>Few components running on the VM, so this is less noisy and more conducive to benchmarking.</li> </ul> <p>Cons:</p> <ul> <li>Only works on AWS. Will require adaptation for other clouds.</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#aws-ec2-with-user-data","title":"AWS EC2 with User Data","text":"<p>This is the strawman: spin up an EC2 instance, install the necessary tools, run the tests, and then tear down the instance. User Data is a way to run commands when the instance is first launched.</p> <p>Additional pros:</p> <ul> <li>None</li> </ul> <p>Additional cons:</p> <ul> <li>There is no good way to get results out of the instance.</li> <li>It is hard to check when tests are done.</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#aws-ec2-with-a-github-self-hosted-runner","title":"AWS EC2 with a GitHub Self-Hosted Runner","text":"<p>This spins up an EC2 instance that runs a GitHub Actions runner. The runner is labeled specifically for the test that spins it up. The Action then runs the test workflow on the runner it just spun up. At the end of the test, the workflow tears down the runner.</p> <p>Additional pros:</p> <ul> <li>Integrated well with GitHub Actions: natively extracts results and continues the workflow when the test is done.</li> </ul> <p>Additional cons:</p> <ul> <li>More complex than EC2 with User Data (but solves that approach's problems).</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#options-considered-kubernetes-based","title":"Options considered - Kubernetes based","text":""},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#common-pros-and-cons_1","title":"Common pros and cons","text":"<p>Pros:</p> <ul> <li>We might be able to reuse this infrastructure for benchmarks with complex Kubernetes workloads.</li> </ul> <p>Cons:</p> <ul> <li>Complex. Need to set up a Kubernetes cluster and all its tooling.</li> <li>Less control over the operating system and AMI.</li> <li>Kubernetes has more components running on the Node (e.g., kubelet) that introduce noise, so this approach is less conducive to benchmarking.</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#spin-up-a-kubernetes-cluster-and-run-the-tests-in-a-pod","title":"Spin up a Kubernetes cluster and run the tests in a pod","text":"<p>This is the approach the Cilium uses for its EKS conformance tests..</p> <p>Additional pros:</p> <ul> <li>Easy to check for completion and extract results (with <code>kubectl</code>).</li> </ul> <p>Additional cons:</p> <ul> <li>More components to set up and tear down (the Kubernetes control plane) which increases the time it takes to run tests and the cost of running tests.</li> <li>Need to write the functionality to extract results ourselves.</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#maintain-a-persistent-kubernetes-cluster-with-actions-runner-controller","title":"Maintain a persistent Kubernetes cluster with <code>actions-runner-controller</code>","text":"<p>following GitHub's \"Autoscaling with self-hosted runners\":</p> <ul> <li>Run a Kubernetes cluster on one of the cloud providers</li> <li>Use GitHub Actions to trigger tests</li> <li>Tests run self hosted on the Kubernetes cluster. The actions-runner-controller seems to be the official controller for this.</li> <li>Each test that requires a specific node type will trigger a runner that only runs on that node type</li> </ul> <p>I believe we can add a nodeSelector in the AutoscalingRunnerSet from the values.yaml when deploying the controller (under template.spec). So this might require a controller deployment per node type.</p> <p>Additional pros:</p> <ul> <li>Very little spin-up and tear-down code, as the controller handles the scaling. This reulsts in simpler Actions, and more reliable cleanup.</li> <li>Tests run on GitHub Runners, so they extract results natively.</li> <li>We can spin up similar clusters on other clouds, and reuse the exact same Actions to run the tests on other clouds' instance types.</li> </ul> <p>Additional cons:</p> <ul> <li>Cluster is relatively complex: needs to anticipate all instance types we want to test on, and add controllers for each. This can be implemented with for loops in a helm chart, but still adds complexity.</li> <li>Cluster would be persistent, so it has ongoing cost, regardless of whether tests are running or not.</li> <li>The cluster would be maintained separately from the tests, so it might be hard to keep them in sync.</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#replicatedcom-compatibility-matrix","title":"Replicated.com Compatibility Matrix","text":"<p>It is a service that spins up full Kubernetes clusters for testing, and bills by usage.</p> <p>Additional pros:</p> <ul> <li>Easy to spin up and tear down clusters.</li> <li>Support for AWS, GCP, Azure, OCI, as well as Openshift, RKE2, and k3s.</li> <li>Might have credits for open source projects (at least with Openshift)</li> </ul> <p>Additional cons:</p> <ul> <li>Needs Kubernetes tooling installed (which complicates the Github Action)</li> <li>Markup over using the clouds directly (although it is small)</li> <li>No spot instance support</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#decision","title":"Decision","text":"<p>We'll use the EC2 + GitHub Actions Runner approach, because it is the simplest way that returns results and is easy to check for completion.</p>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#consequences","title":"Consequences","text":""},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#positive","title":"Positive","text":"<ul> <li>Can write the entire test as a GitHub Action.</li> <li>The same approach can be used for benchmarking.</li> <li>Can use AWS credits to run tests.</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#negative","title":"Negative","text":"<ul> <li>We are currently just enabling AWS. To run on other clouds, the setup and cleanup would need to be updated.</li> </ul>"},{"location":"architecture/decisions/2025-01-02-aws-test-runners/#risks","title":"Risks","text":"<ul> <li>Making cleanup bulletproof would require iteration, which could lead to orphaned runners and their associated costs in the interim.</li> </ul>"},{"location":"architecture/decisions/2025-02-18-getting-cmt-measurements/","title":"2025-02-18: Collecting Intel CMT Measurements","text":""},{"location":"architecture/decisions/2025-02-18-getting-cmt-measurements/#context","title":"Context","text":"<p>We need to collect Intel CMT (Cache Monitoring Technology) measurements at millisecond granularity for containers in a cloud-native environment. </p>"},{"location":"architecture/decisions/2025-02-18-getting-cmt-measurements/#decision","title":"Decision","text":"<p>We will build a kernel module that interacts directly with Intel RDT MSRs (Model Specific Registers) to configure and read CMT measurements.</p>"},{"location":"architecture/decisions/2025-02-18-getting-cmt-measurements/#rationale","title":"Rationale","text":"<p>We considered three approaches for collecting CMT measurements:</p> <ol> <li> <p>Using Linux perf counters: After investigating the Intel CMT-CAT repository and relevant Linux kernel code, we found that although the Intel software repository seemed to support perf counters for CMT, the Linux kernel did not actually implement this. Therefore, using perf was not a viable option.</p> </li> <li> <p>Using the resctrl filesystem interface: The Linux kernel's resctrl subsystem provides a filesystem-based interface for configuring Intel RDT and reading measurements. However, this approach has several drawbacks:</p> </li> <li>Collecting measurements at millisecond granularity through the filesystem interface for all containers would be complex and potentially inefficient due to the overhead of system calls. </li> <li> <p>Resctrl is based on tasks and processes rather than containers. To use resctrl, we would need to build a system to monitor container lifecycle events and configure resctrl accordingly, which would add complexity and potential gaps in measurement.</p> </li> <li> <p>Building a kernel module to interact with MSRs directly: This approach offers several advantages:</p> </li> <li>By interacting with MSRs directly, the kernel module can read CMT information with very low overhead, without the layers of the filesystem interface.</li> <li>The kernel module can probe container lifecycle tracepoints to allocate RMIDs (Resource Monitoring IDs) and assign them to containers automatically.</li> <li>This approach enables a cloud-native solution that seamlessly measures containers as they are created.</li> </ol> <p>Given these considerations, we chose to build a kernel module that interacts with Intel RDT MSRs directly. This approach provides the best performance, flexibility, and compatibility with a cloud-native container environment.</p>"},{"location":"architecture/decisions/2025-02-18-getting-cmt-measurements/#consequences","title":"Consequences","text":"<p>Building a kernel module for CMT measurement has the following consequences:</p> <ul> <li>We will need to maintain the kernel module code and ensure compatibility with different Linux kernel versions.</li> <li>Users will need to load the kernel module to enable CMT measurement collection.</li> <li>We will have tight integration with container lifecycle events, enabling seamless measurement of containers.</li> <li>We can achieve low-overhead, millisecond-granularity measurement collection, meeting our performance requirements.</li> </ul>"},{"location":"architecture/decisions/2025-02-18-getting-cmt-measurements/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"architecture/decisions/2025-02-18-getting-cmt-measurements/#appendix-a-intel-cmt-cat-summary","title":"Appendix A: intel-cmt-cat summary","text":"<p>The <code>perf_monitoring.c</code> file:</p> <ul> <li>Checks if perf is available by checking if <code>/proc/sys/kernel/perf_event_paranoid</code> exists.</li> <li>Checks if RDT exists by reading <code>/sys/devices/intel_cqm/type</code><ul> <li>if it exists, its value (as integer) is the perf <code>type</code> field</li> <li>traverses <code>/sys/devices/intel_cqm/events</code> for events <code>llc_occupancy</code>, <code>local_bytes</code>, <code>total_bytes</code></li> <li>their value is parsed to get the <code>config</code> field of the perf struct</li> <li>the same file with extension <code>.scale</code> is used to read a <code>double</code> scale</li> </ul> </li> </ul>"},{"location":"architecture/decisions/2025-02-18-getting-cmt-measurements/#mentions-of-using-the-perf-command-line","title":"Mentions of using the perf command line","text":"<p>Here are references from the web for monitoring RDT using perf. However note that we found that the patches discussed in these references were not present in the Linux kernel whose code we checked (6.13.2) and appear to have not been merged into the kernel originally.</p> <p>A 2017 forum post was able to view events with <code>perf stat</code> as events:</p> <p><code>intel_cqm/llc_occupancy , intel_cqm/llc_local_bytes/,intel_cqm_total_bytes/</code></p> <p>(the last value seems to have a typo replacing <code>/</code> with <code>_</code>)</p> <p>An Intel/Kanaka Juvva presentation at LinuxCon'2015 shows per-application memory bandwidth monitoring with <code>perf</code> (slide 11):</p> <p>Two perf events are exported to userland - LOCAL_BW   - perf stat \u2013e intel_cqm/llc_local_bw/ -a \u201cmy_application\u201d - TOTAL_BW   - perf stat \u2013e intel_cqm/llc_total_bw/ -a \u201cmy_application\u201d</p> <p>A 2016 Kanaka Juuva presentation: - further mentions LLC Occupancy - shows memory bandwidth benchmark results - shows more process-based CLI examples, by PID:</p> <ul> <li>LLC_OCCUPANCY</li> <li>perf stat \u2013e intel_cqm/llc_occupancy/ -p \u201cpid of my_application\u201d</li> <li>discusses cgroups-based measurements. This might have been before the switch from cgroup to resctrl.</li> </ul>"},{"location":"architecture/decisions/2025-02-18-getting-cmt-measurements/#appendix-b-a-journey-through-intel-cmt-cat","title":"Appendix B: A journey through intel-cmt-cat","text":"<p>The intel-cmt-cat repo documentation suggests perf can read CMT data as well (table 5 in README).</p> <p>In this section, we look into how intel-cmt-cat uses perf, and document its usage so we can support that alongside the other counters.</p> <p>We start with the <code>pqos</code> CLI tool. Its command line parameters set up calls into the library in <code>lib/</code>:</p> <ul> <li><code>main</code> calls <code>selfn_monitor_cores</code> on the <code>-m</code> command line option.</li> <li><code>parse_monitor_cores</code> parses the <code>-m</code> command line option.</li> <li><code>parse_monitor_group</code> parses a string from the command line to a list of cores or pids, and calls <code>grp_add</code> on each.</li> <li><code>grp_add</code> allocates a <code>struct mon_group</code> called <code>new_grp</code> on the stack, then adds the core/pid/channel/etc. to the group using <code>grp_set_*</code>, and then appends it to a global variable <code>sel_monitor_group</code>.</li> <li>later, <code>main</code> calls <code>monitor_setup</code></li> <li><code>monitor_setup</code> calls the library API depending on the type of monitor. For cores, it calls <code>pqos_mon_start_cores</code>.</li> </ul> <p>Going into the library:</p> <ul> <li><code>pqos_mon_start_cores</code> calls <code>pqos_mon_start_cores_ext</code> (which also has an opt parameter)</li> <li><code>pqos_mon_start_cores_ext</code> checks input validity and then makes an <code>API_CALL(mon_start_cores...)</code></li> <li><code>API_CALL</code> is a macro that accesses a virtual table of monitoring operations called <code>api</code> in <code>api.c</code>. <ul> <li>This <code>api</code> variable is initialized in <code>api_init</code> to either the OS interface or MSR interface (these are mentioned in the repo's README).</li> <li>In the OS interface, the <code>mon_start_cores</code> function pointer is initialized to point to <code>os_mon_start_cores</code>.</li> </ul> </li> <li><code>os_mon_start_cores</code> validates the input, the available monitoring capabilities, and ensures the monitoring hadn't already started, and calls <code>os_mon_start_events</code>.</li> <li><code>os_mon_start_events</code>:<ul> <li>runs <code>perf_mon_is_event_supported</code> on every event, and if so, calls <code>perf_mon_start</code>.</li> <li>otherwise, checks <code>resctrl_mon_is_event_supported</code> and if so performs <code>resctrl_mon_start</code>.</li> </ul> </li> </ul> <p>Let's explore the flow that checks perf for supported events:</p> <ul> <li><code>perf_mon_is_event_supported</code> calls <code>get_supported_event</code>.</li> <li><code>get_supported_event</code> looks up the event in a global <code>events_tab</code>.<ul> <li>the first event in <code>events_tab</code> is <code>llc_occupancy</code>.</li> </ul> </li> </ul> <p>Initialization of perf monitoring in <code>perf_mon_init</code>:</p> <ul> <li>if <code>/proc/sys/kernel/perf_event_paranoid</code> exists, enables the PMU events (cycles, instructions, IPC, LLC misses, LLC references).</li> <li><code>set_arch_event_attrs</code> sets the <code>attr</code> field on PMU events. The <code>attr</code> field is a <code>struct perf_event_attr</code> (from the linux API).</li> <li><code>set_mon_type</code> reads <code>/sys/devices/intel_cqm/type</code> as an integer into the global variable <code>os_mon_type</code>. This int is then used in the perf attr as its <code>type</code> field in <code>set_rdt_event_attrs</code>.</li> <li><code>set_mon_events</code> then traverses the directory <code>/sys/devices/intel_cqm/events</code>. <ul> <li>For each file, it tries to find an entry in <code>events_tab</code> whose <code>name</code> field is the same as the file name. </li> <li>For every match, it calls <code>set_rdt_event_attrs</code>.</li> </ul> </li> <li><code>set_rdt_event_attrs</code><ul> <li>reads the file</li> <li>assumes the contents has a <code>=</code>, discards everything before the first <code>=</code> and parses the rest as an integer. this will be <code>attrs.config</code></li> <li>reads another file filename+<code>.scale</code> suffix</li> <li>parses it as a double. this will be the event's <code>scale</code></li> </ul> </li> </ul> <p>So far, we covered initialization and checking event availability. Now let's see how the library configures the kernel to start monitoring:</p> <ul> <li><code>perf_event_open</code> makes the syscall via <code>syscall(__NR_perf_event_open, attr, pid, cpu, group_fd, flags)</code></li> </ul>"},{"location":"ci/aws-setup/","title":"AWS Setup for CI Testing","text":"<p>Our CI system needs to test the collector on various AWS instance types to validate hardware-specific features. We use GitHub Actions to dynamically spin up EC2 instances with specific hardware configurations, run our tests, and then tear down the instances.</p>"},{"location":"ci/aws-setup/#overview","title":"Overview","text":"<p>The CI system uses two key community actions: - machulav/EC2-github-runner: Manages ephemeral EC2 instances for test execution - aws-actions/configure-aws-credentials: Handles AWS authentication through GitHub's OIDC provider</p> <p>Each test workflow creates a dedicated GitHub Actions runner on a fresh EC2 instance. This ensures our tests run in clean environments with specific hardware configurations.</p>"},{"location":"ci/aws-setup/#aws-account-setup","title":"AWS Account Setup","text":"<p>We maintain a dedicated AWS account for CI testing to isolate these resources from production environments. This separation provides clearer cost tracking and stronger security boundaries.</p>"},{"location":"ci/aws-setup/#administrative-access","title":"Administrative Access","text":"<p>After creating the dedicated CI testing account, set up administrative access:</p> <ol> <li>In the root account, navigate to IAM Identity Center</li> <li>Create a new user in the IAM Identity Center</li> <li>Create a Permission Set or use the existing \"Administrator Access\" permission set</li> <li>Note: \"Power User Access\" is insufficient as it doesn't allow IAM role creation</li> <li>Assign the user to the CI testing account with the Administrator Access permission set</li> </ol> <p>The Administrator Access permission set is required for subsequent IAM configuration steps. While Power User Access might seem sufficient, it lacks the necessary permissions for creating IAM roles needed for GitHub Actions integration.</p>"},{"location":"ci/aws-setup/#iam-configuration","title":"IAM Configuration","text":"<p>First, configure GitHub as an OIDC provider to enable secure authentication:</p> <ol> <li>Open the IAM console</li> <li>Navigate to \"Identity Providers\" and add a new provider</li> <li>Select \"OpenID Connect\"</li> <li>Use <code>https://token.actions.githubusercontent.com</code> as the provider URL</li> <li>Set the audience to <code>sts.amazonaws.com</code></li> </ol> <p>Next, create an IAM role for GitHub Actions:</p> <ol> <li>Create a new role</li> <li>Set the Trusted entity type to Web Identity</li> <li>Select the GitHub OIDC provider as the trust entity</li> <li>Fill in the org (<code>unvariance</code>) and repo (<code>collector</code>), not filling in the branch</li> <li>Add no permissions (we will do this in a moment)</li> <li>Name the role, e.g., <code>github-actions-collector</code>.</li> <li>Verify the generated trusted entities to be:</li> </ol> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Federated\": \"arn:aws:iam::&lt;ACCOUNT-ID&gt;:oidc-provider/token.actions.githubusercontent.com\"\n            },\n            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"token.actions.githubusercontent.com:aud\": \"sts.amazonaws.com\"\n                },\n                \"StringLike\": {\n                    \"token.actions.githubusercontent.com:sub\": \"repo:unvariance/collector:*\"\n                }\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"ci/aws-setup/#ec2-permissions","title":"EC2 Permissions","text":"<p>The IAM role needs permissions to manage EC2 instances and request Spot instances. Attach a policy with these minimum permissions:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:RunInstances\",\n                \"ec2:TerminateInstances\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeInstanceStatus\",\n                \"ec2:RequestSpotInstances\",\n                \"ec2:CancelSpotInstanceRequests\",\n                \"ec2:DescribeSpotInstanceRequests\",\n                \"ec2:DescribeSpotPriceHistory\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"ec2:CreateTags\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"ec2:CreateAction\": [\n                        \"RunInstances\",\n                        \"RequestSpotInstances\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"ci/aws-setup/#allowing-spot-instances","title":"Allowing Spot instances","text":"<p>According to AWS spot instance role docs:</p> <p>Under most circumstances, you don't need to manually create a service-linked role. Amazon EC2 creates the\u00a0AWSServiceRoleForEC2Spot\u00a0service-linked role the first time you request a Spot Instance using the console.</p> <p>And later:</p> <p>If you use the AWS CLI or an API to request a Spot Instance, you must first ensure that this role exists.</p> <p>So to enable Spot instances, we recommend making a request on the account using the console, which creates the role, and then canceling the request.</p>"},{"location":"ci/aws-setup/#instance-quotas","title":"Instance quotas","text":"<p>We found default EC2 and Spot quotas to be smaller than most machines that make the PMU available to VMs. We requested an increase to 192 cores.</p> <p>Quota requests can be made through this page. Note that there are separate quotas for On-Demand and for Spot.</p>"},{"location":"ci/aws-setup/#network-configuration","title":"Network Configuration","text":"<p>Create a dedicated VPC for CI testing:</p> <ol> <li>Create a new VPC with a single public subnet</li> <li>Set up appropriate security groups to allow:</li> <li>Outbound traffic on port 443 for communication with GitHub</li> </ol>"},{"location":"ci/aws-setup/#repository-variables","title":"Repository variables","text":"<p>Configure the repository with the following secrets that can be used in Actions:</p> <ul> <li><code>AWS_ROLE_ARN</code>: the ARN of the role that allows running and terminating instances</li> <li><code>AWS_REGION</code>: the region where we'll run runners</li> <li><code>AWS_SUBNET_ID</code>: the subnet ID, needs to be in <code>AWS_REGION</code></li> <li><code>AWS_SECURITY_GROUP_ID</code>: the name of the security group that allows runners to pull jobs</li> <li><code>REPO_ADMIN_TOKEN</code>: see below</li> </ul>"},{"location":"ci/aws-setup/#getting-a-token-for-ec2-github-runner","title":"Getting a token for ec2-github-runner","text":"<p>To register runners with GitHub, the <code>machulav/ec2-github-runner</code> action needs a GitHub token that has permissions to modify the the repository's set of self hosted runners. This might be transferable to user accounts but I haven't checked.</p> <p>A discussion thread implies that finer-grained permissions might be available, where a token would only be able to configure runners rather than full Administration privileges, but it didn't work.</p> <ol> <li>Configure your organization to allow fine-grained tokens. In Organization Settings -&gt; Third-party Access -&gt; Personal access tokens -&gt; Settings, allow access via fine-grained personal access tokens</li> <li>Create a fine-grained personal access token here: https://github.com/settings/personal-access-tokens/new</li> <li>Set the resource owner to be the organization</li> <li>Set the permission scope to \"Only select repositories\", and select the repo with the GitHub Action</li> <li>In Repository permissions, add \"Administration\" (read and write)</li> </ol>"},{"location":"ci/aws-setup/#github-workflow-configuration","title":"GitHub Workflow Configuration","text":"<p>For an example workflow, adapted from the ec2-github-runner README and configure-aws-credentials README example, see <code>/.github/workflows/aws-runner-template.yaml</code>.</p>"}]}